# -*- coding: utf-8 -*-
"""spendingbrazil.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mPehRxHlqbii6nxF-0cJP80cDVDswfo7

##Firstly, data cleaning should be done.
##Train and test data is created according to the size of the data we have (we assume that customers are tagged).
##We start using the logistic regression algorithm, which is the most basic algorithm.We are building our logistic regression model with the data we have
##An error of our test data with the model  created

## Logistic Regression  give us 49% of accuracy
## Random ForestClsssifier give us 72% of accu racy.
##XBosst:88%of accuracy
##To improve the results was used tuned parameters, cross-validation and Hyper Parameter Tuning (RandomizedSearchCV)
"""





#strong relationship between

#import libraries 
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt 
import numpy as np
import pandas as pd
import seaborn as sns

#read the csv file 
df = pd.read_csv('MiBolsillo.csv',encoding = 'unicode_escape',sep=';')
df.dataframeName = 'MiBolsillo.csv'
nRow, nCol = df.shape
print(f'There are {nRow} rows and {nCol} columns')

df.head()

#change name of the columns 'id','branc
df.columns = ['id','city','state','age','gender','total_credit_card_limit','current_available_limit' ,'date','amount','category_expense','purchase_city','purchase_country']

#converting the date variable to date_time format
df.date = pd.to_datetime(df.date,dayfirst=True)
df.head()

#create a dataframe as customers with old and new customer names
customers = pd.DataFrame(df.id.unique())
customers.columns = ['old_customer_name']
new_list = list(range(1, len(customers)+1))
customers['new_customer_name'] = new_list
customers

# we need to float the type of amount variable
for i in range(0,len(df.amount)):
    df.amount[i] = df.amount[i].replace('.','').replace(',','.')

#I export values containing '-' in the amount variable as df_amount_nan
df_amount_nan = df[df.amount == ' -   ']

# There are '-' values in the amount column. I am deleting theese rows because we can't convert it to the float type with these values
df = df[df.amount != ' -   ']

# Now the amount variable is ready to convert to float type.
df.amount = df.amount.astype(float)

#converting the date variable to date_time format

df.date = pd.to_datetime(df.date,dayfirst=True)

#convert the gender variable to dummy_variable
dms = pd.get_dummies(df['gender'])
df = pd.concat([df,dms],axis=1)
df.drop(['gender', 'M'], axis=1,inplace=True)
df.rename(columns={'F': 'Female'}, inplace=True)
df

# I look at the total spending of each customer
total = df.groupby('id')[['amount']].sum().sort_values('amount',ascending=False)
total.rename(columns={'amount': 'Total_spending'}, inplace=True)
total

# I want to look at the total expense in each category
categ = df.category_expense.value_counts().sort_values(ascending=False)


plt.figure(figsize=(15,10))
sns.barplot(x=categ.index,y=categ.values)
plt.xlabel('Cataegories')
plt.ylabel('Count')
plt.title("Cataegories Count")
plt.xticks(rotation= 45);

#the number of transactions each client has with a credit card
freq = df.groupby('id')[['amount']].count().sort_values('amount',ascending=False)
freq.rename(columns={'amount': 'Frequency'}, inplace=True)
freq

#look at the total spending of each customer
plt.figure(figsize=(15,10))
sns.barplot(x=total.index,y=total['Total_spending'])
plt.xlabel('Customers')
plt.ylabel('Total Spending')
plt.title("Total Spending vs Customers")
plt.xticks(rotation= 45);

# Scatter and density plots
def plotScatterMatrix(df, plotSize, textSize):
    df = df.select_dtypes(include =[np.number]) # keep only numerical columns
    # Remove rows and columns that would lead to df being singular
    df = df.dropna('columns')
    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values
    columnNames = list(df)
    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots
        columnNames = columnNames[:10]
    df = df[columnNames]
    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')
    corrs = df.corr().values
    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):
        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)
    plt.suptitle('Scatter and Density Plot')
    plt.show()

#correlation between variables 
plotScatterMatrix(df, 15, 10)

# Distribution graphs (histogram/bar graph) of column data
def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):
    nunique = df.nunique()
    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values
    nRow, nCol = df.shape
    columnNames = list(df)
    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow
    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')
    for i in range(min(nCol, nGraphShown)):
        plt.subplot(nGraphRow, nGraphPerRow, i + 1)
        columnDf = df.iloc[:, i]
        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):
            valueCounts = columnDf.value_counts()
            valueCounts.plot.bar()
        else:
            columnDf.hist()
        plt.ylabel('counts')
        plt.xticks(rotation = 90)
        plt.title(f'{columnNames[i]} (column {i})')
    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)
    plt.show()
# Correlation matrix

plotPerColumnDistribution(df, 10, 5)

df.corr()

#correlation in a heatmap
import seaborn as sns
corr = df.corr()
sns.heatmap(corr, 
            xticklabels=corr.columns.values,
            yticklabels=corr.columns.values)

"""###Total credit limit and current avaible limit represent strong relation
### input in the machine learning 
"""

df["id"].replace(customers['old_customer_name'].values,customers['new_customer_name'].values, inplace=True)

df.info()

df.date = pd.to_datetime(df.date,dayfirst=True)

#want to look at the total spending of each customers
freq = df.groupby('id')[['amount']].count().sort_values('amount',ascending=False)
freq.rename(columns={'amount': 'Frequency'}, inplace=True)
freq

total = df.groupby('id')[['amount']].sum().sort_values('amount',ascending=False)
total.rename(columns={'amount': 'Total_spending'}, inplace=True)
total

plt.figure(figsize=(15,10))
sns.barplot(x=total.index,y=total['Total_spending'])
plt.xlabel('Customers')
plt.ylabel('Total Spending')
plt.title("Total Spending vs Customers")
plt.xticks(rotation= 45);

"""### As it is seen, the 19th customer is the person who spends the most among the customers.
### We are going to analize the exepenses according to covid crisis.
"""

# pre covid
pre_covid = df[(df.date > '2020-01-01') & (df.date < '2020-03-18')]

#covid
covid = df[(df.date >= '2020-03-18')]

covid.head()

covid_freq = covid.groupby('id')[['age']].count().sort_values('age',ascending=False)
covid_freq.columns = ['frequency']
print('average number of transactions frequency: ',covid_freq.frequency.mean())
print('max number of transactions frequency: ',covid_freq.frequency.max())
print('min number of transactions frequency: ',covid_freq.frequency.min())

plt.figure(figsize=(15,10))
sns.barplot(x=covid_freq.index,y=covid_freq['frequency'])
plt.xlabel('Customers')
plt.ylabel('Frequency of use')
plt.title("The frequency of transactions made by customers during the covid")
plt.xticks(rotation= 45);

covid_freq

pre_covid_freq = pre_covid.groupby('id')[['age']].count().sort_values('age',ascending=False)
pre_covid_freq.columns = ['frequency']
print('average number of transactions frequency: ',pre_covid_freq.frequency.mean())
print('max number of transactions frequency: ',pre_covid_freq.frequency.max())
print('min number of transactions frequency: ',pre_covid_freq.frequency.min())

plt.figure(figsize=(15,10))
sns.barplot(x=pre_covid_freq.index,y=pre_covid_freq['frequency'])
plt.xlabel('Customers')
plt.ylabel('Frequency of use')
plt.title("The frequency of transactions made by customers before the covid")
plt.xticks(rotation= 45);

pre_covid_freq

covid['category_expense'].value_counts()

essential_list = ['FARMACIAS','VAREJO','HOSP E CLINICA','SUPERMERCADOS','POSTO DE GAS','TRANS FINANC']

non_essential_list = ['SERVI\x82O','M.O.T.O.','ARTIGOS ELETRO','LOJA DE DEPART','VESTUARIO','SEM RAMO','MAT CONSTRUCAO','RESTAURANTE','CIA AEREAS','MOVEIS E DECOR','JOALHERIA','AGENCIA DE TUR','HOTEIS','AUTO PE AS','INEXISTENTE','']

covid[covid.category_expense.isin(essential_list)]['category_expense'].value_counts()

print('Total spending during covid: ',covid['amount'].sum(),'Brazillian R')
print('Total spending in essential category during covid',covid[covid.category_expense.isin(essential_list)]['amount'].sum(),'Brazillian R')
print('Essential : %',covid[covid.category_expense.isin(essential_list)]['amount'].sum() * 100 / covid['amount'].sum())
essential_covid = covid[covid.category_expense.isin(essential_list)]['amount'].sum() * 100 / covid['amount'].sum()

covid[covid.category_expense.isin(non_essential_list)]['category_expense'].value_counts()

# % of non essential
print('Total spending during covid: ',covid['amount'].sum(),'Brazillian R')
print('Total spending in non essential category during covid',covid[covid.category_expense.isin(non_essential_list)]['amount'].sum(),'Brazillian R')
print('Non - Essential : %',covid[covid.category_expense.isin(non_essential_list)]['amount'].sum() * 100 / covid['amount'].sum())
non_essential_covid = covid[covid.category_expense.isin(non_essential_list)]['amount'].sum() * 100 / covid['amount'].sum()

#%%of essential
covid[covid.category_expense.isin(essential_list)]['category_expense'].value_counts()[:3]

pre_covid[pre_covid.category_expense.isin(non_essential_list)]['category_expense'].value_counts()[:3]

# The lowest paid spending and category of each customers before covid

for i in range(1,30):
    print(pre_covid[pre_covid['id'] == i].sort_values('amount')[:3][['category_expense','amount']].set_index(pre_covid[pre_covid['id'] == i]['id'][:3]))

# The lowest paid spending and category of each customers before covid

for i in range(1,30):
    print(pre_covid[pre_covid['id'] == i].sort_values('amount',ascending=False)[:3][['category_expense','amount']].set_index(pre_covid[pre_covid['id'] == i]['id'][:3]))

freq = pd.concat([covid_freq.sort_index(),pre_covid_freq.sort_index()],axis=1)
freq.columns = ['covid_freq','pre_covid_freq']

# The frequency of using credit card for each customer before covid vs during covid


fig, ax = plt.subplots(2,2,sharey=True)


ax[0,0].plot(pre_covid_freq.sort_index().index,pre_covid_freq.sort_index().values,color='g',marker='o')
ax[0,0].set_title('Pre Covid')


ax[0,1].plot(covid_freq.sort_index().index,covid_freq.sort_index().values,color='b',marker='o')
ax[0,1].set_title('Covid')

plt.show();

import plotly.graph_objs as go
#import chart_studio.plotly as py


fig = go.Figure()
fig.add_trace(go.Box(y=freq.covid_freq, name='The frequency of using credit card for each customer during covid',
                marker_color = 'indianred'))
fig.add_trace(go.Box(y=freq.pre_covid_freq, name = 'The frequency of using credit card for each customer before covid',
                marker_color = 'lightseagreen'))

fig.show()

from plotly.offline import init_notebook_mode, iplot, plot
import plotly.graph_objs as go


# Creating trace1
trace1 = go.Scatter(
                    x = freq.index,
                    y = freq.covid_freq,
                    mode = "lines",
                    name = "Covid Freq",
                    marker = dict(color = 'rgba(16, 112, 2, 0.8)'),
                    text= freq.covid_freq)
# Creating trace2
trace2 = go.Scatter(
                    x = freq.index,
                    y = freq.pre_covid_freq,
                    mode = "lines+markers",
                    name = "Pre Covidreq",
                    marker = dict(color = 'rgba(80, 26, 80, 0.8)'),
                    text= freq.pre_covid_freq)
data = [trace1, trace2]
layout = dict(title = 'The frequency of using credit card for each customer before covid vs during covid',
              xaxis= dict(title= 'Customers',ticklen= 5,zeroline= False)
             )
fig = dict(data = data, layout = layout)
iplot(fig)

df.corr()

"""###I would take the variables 'total_credit_card_limit', 'current_available_limit' and 'age'. When we look at the correlation matrix, the relationship between them is the most in these 3 variables to evaluate the risk"""

df

y = df['total_credit_card_limit'].astype(int)
x = df[['age','current_available_limit']]

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(x))

X_train, X_pre_test, y_train, y_pre_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Focus that we are using X_pre_test and y_pre_test. Also test_size is fixed to 0.5

X_val, X_test, y_val, y_test = train_test_split(X_pre_test, y_pre_test , test_size=0.5, random_state=123)

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()

model.fit(X_train, y_train)

pred = model.predict(X_test)

from sklearn.metrics import accuracy_score

score = accuracy_score(y_test, pred)
print('The Accuracy Score is', score)

from sklearn.metrics import confusion_matrix, classification_report

accuracy = confusion_matrix(y_test, pred)
print('The Accuracy of Classifier: \n', accuracy)

"""###The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.The best value is 1 and the worst value is 0.

###The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label a negative sample as positive.

###The F-1 score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.
"""

matrix = classification_report(y_test, pred)
print('Classification Report: \n', matrix)

#from sklearn.metrics import roc_auc_score, roc_curve

#auc_score = roc_auc_score(y_test, pred)
#auc_score

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier()

rfc.fit(X_train, y_train)

rfc_pred = rfc.predict(X_test)

"""##The accuracy_score function computes the accuracy, either the fraction (default) or the count (normalize=False) of correct predictions.

##In multilabel classification, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.
"""

score = accuracy_score(y_test, rfc_pred)
print('The Accuracy Score is', score)

#XgBoost Classifier
import xgboost

#Hyper parameter tuning models
from sklearn.model_selection import RandomizedSearchCV , GridSearchCV

from sklearn.model_selection import train_test_split

X_train , X_test, y_train , y_test = train_test_split(X,y,test_size=0.3, random_state=42)

baseline_model = xgboost.XGBClassifier()

#Fitting data to the model
baseline_model.fit(X_train,y_train)

#Training accuracy
baseline_model.score(X_train,y_train)

#Predictions
baseline_preds = baseline_model.predict(X_test)

#Evaluation Metrics
from sklearn.metrics import confusion_matrix , classification_report

#Confusion metrics
print(confusion_matrix(y_test,baseline_preds))

#Classification Report
print(classification_report(y_test,baseline_preds))

"""##Hyper Parameter Tuning (RandomizedSearchCV)"""

xgb_params = {
            'learning_rate' : [0.05,0.10,0.15,0.20,0.25,0.30],
            'max_depth'     : [3,4,5,6,8,10,12,15],
            'min_child_weight' : [1,3,5,7],
            'gamma' : [0.0,0.1,0.2,0.3,0.4],
            'colsample_bytree' : [0.3,0.4,0.5,0.7]    
            }

rs_xgb_model = RandomizedSearchCV(xgboost.XGBClassifier(),xgb_params,n_iter=5,scoring = 'accuracy',cv=5,verbose=2)

rs_xgb_model.fit(X_train,y_train)

#Best Parameters
rs_xgb_model.best_params_

#Best Estimators
rs_xgb_model.best_estimator_

# Training Score
rs_xgb_model.score(X_train,y_train)

# Testing score
rs_xgb_model.score(X_test,y_test)

"""##Tuned Model"""

#Retrieved from best estimators (Randomized search cv)
xgb_tuned_model=xgboost.XGBClassifier(min_child_weight=1,max_depth=10,learning_rate=0.05, gamma=0.1,colsample_bytree=0.5)

#Fitting data
xgb_tuned_model.fit(X_train,y_train)

#Training score
xgb_tuned_model.score(X_train,y_train)

#Testing score
xgb_tuned_model.score(X_test,y_test)

#Predictions
tuned_preds = xgb_tuned_model.predict(X_test)

#Confusion matrix
print(confusion_matrix(y_test,tuned_preds))

#Classification Report
print(classification_report(y_test,tuned_preds))

"""##Cross Validation Score"""

from sklearn.model_selection import cross_val_score

cv_score = cross_val_score(xgb_tuned_model,X_train,y_train,cv=10)

np.mean(cv_score)

xgb_tuned_model.fit(X,y)

#evaluate score
xgb_tuned_model.score(X,y)

"""###Return the mean accuracy on the given test data and labels.In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.

###References
###https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=score#sklearn.ensemble.RandomForestClassifier.score
###https://scikit-learn.org/stable/modules/model_evaluation.htmlaccuracy-score
####https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html?highlight=from%20sklearn%20model_selection%20import%20cross_val_score
###https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support


###Brier1950G. Brier, Verification of forecasts expressed in terms of probability, Monthly weather review 78.1 (1950)

###Bella2012(1,2)Bella, Ferri, Hernández-Orallo, and Ramírez-Quintana “Calibration of Machine Learning Models” in Khosrow-Pour, M. “Machine learning: concepts, methodologies, tools and applications.” Hershey, PA: Information Science Reference (2012).

###Flach2008Flach, Peter, and Edson Matsubara. “On classification, ranking, and probability estimation.” Dagstuhl Seminar Proceedings. Schloss Dagstuhl-Leibniz-Zentrum fr Informatik (2008).
"""